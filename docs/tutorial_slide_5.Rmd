---
title: "第五章 単回帰分析"
subtitle: "<small>A tutorial for regression analysis</small>"
author: "阿部景太"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default,ninjutsu,useR-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      coutIncrementalSlides: false
      ratio: 16:9
      
---

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    font-family: "メイリオ" ;
    padding: 1em 4em 1em 4em;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
pacman::p_load(
  tidyverse,
  knitr,
  kableExtra,
  patchwork,
  cols4all
)

## options(scipen=999)
```

# 期待値と因果効果

- 因果効果を条件付き期待値の差で表す

$$
  E[Y|X=1] - E[Y|X=0]
$$

$X$ が「朝ごはんを食べる」という確率変数。 $X=1$ なら「食べる」, $X=0$ なら食べない。

---

# 外的条件が揃っていない場合

外的条件 $C$ が揃っていない場合、確率変数 $C$ が取りうる値 $c$ を使って揃える。

$$
  E[Y|X=1, C=c] - E[Y|X=0, C=c]
$$

政策を示す変数(政策変数)と成果を示す変数(成果変数)の関係のみをモデル化するものが**単回帰分析**。外的条件を共変量で制御しながら因果効果を探る回帰分析を**重回帰分析**と呼ぶ。


---

# 回帰分析

**回帰分析**は、成果変数の期待値 $E[Y|X]$ を説明変数の関数として**モデル化**

$$
 E[Y|X] = f(X)
$$

---

# 回帰分析：単回帰モデル

単回帰モデルは、政策変数の実現値で条件付けした成果変数の期待値を次のようにモデル化する

$$
  E[Y|X=x] = \beta_0 + \beta_1 x
$$

---

朝ごはんの例では、朝ごはんと食べた場合の期待値( $X=1$ )は
$$
 E[Y|X=1] = \beta_0 + \beta_1
$$

朝ごはんと食べない場合の期待値( $X=0$ )は
$$
 E[Y|X=0] = \beta_0
$$

因果効果は

$$
 E[Y|X=1] - E[Y|X=0] = (\beta_0 + \beta_1) - \beta_0 = \beta_1
$$

---

# 誤差項

実際のデータの期待値からのズレの項を**誤差項**と呼ぶ

$$
 Y = \beta_0 + \beta_1 X + U
$$

$Y$ , $X$ , $U$ の3つの確率変数が従う母集団分布における関係をモデル化 $\rightarrow$ 母集団モデル(真のモデル)   


---

# 単回帰モデル

データを用いて推定する（観測番号がつく）

$$
 Y_i = \beta_0 + \beta_1 X_i + U_i
$$

$\beta_0,\beta_1$ : 回帰パラメター
$Y$ ：被説明変数/従属変数
$X$ ：説明変数/独立変数

---

# パラメター化 (Parameterization)

単回帰モデルは成果変数 $Y$ の条件付き期待値をモデル化したもの

$E[Y|X]$ は $X$ の実現値に依存する関数として条件付き期待値関数と呼ばれる。

この関数を有限個(この場合は2個)のパラメターで表現することをパラメター化と呼ぶ。

他にもモデル化する方法はある (例 $E[Y|X=x] = \beta_0 + \beta_1 x + \beta_2 x^2$)

1次関数近似のメリット：パラメターの数が少ない。

---

# 因果関係を示すための条件

外的条件が完全に制御されている状況から得られたデータ（例：実験）であれば、得られたパラメター $\beta_1$ を**因果効果**として解釈することは可能

しかし、外的条件が制御されていない場合は、単なる**相関関係**を表しているに過ぎない。

これが因果効果であると示すにはいくつかの条件を満たす必要

---

# 因果関係のための仮定1

説明変数 $X$ と 誤差項 $U$ は平均独立 : $E[U|X]=E[U]$

例：朝ごはんを食べてないグループの誤差項が食べたグループの誤差項より低いなら、外的条件が揃っているとはいえない。

$$
 E[U_i | X_i = 1] > E[U_i | X_i = 0]
$$

---

# 因果関係のための仮定2

誤差項Uの母平均は0 : $E[U] = 0$

これによって、条件付き期待値を1次関数のモデルによって表すことができる。

例題：次の関係が成り立つことを、仮定２つを応用して示せ

$$
 E[Y|X] = \beta_0 + \beta_1 X
$$

---

# 例題の解答

$Y$ の真のモデルを代入

$$
 E[Y|X] = E[\beta_0 + \beta_1 X + U | X]
$$
条件付き期待値の性質(4)を使う (p.56)

$$
 = E[\beta_0|X] + E[\beta_1 X |X] + E[U|X]
$$

条件付き期待値の性質(1)と(5)、そして仮定1、仮定2を使う

$$
 = \beta_0 + \beta_1 X + E[U] = \beta_0 + \beta_1 X
$$


---

# 最小二乗法

- パラメター化されたモデルのパラメターを**推定**する。  
  - どうやって？
  - **最小二乗法** (OLS: ordinary least squares) 

- データ：標本サイズが $n$ 
  - 分析したい母集団からの無作為抽出データ
  - $\{(x_i, y_i)\}$ for $i=1,\ldots,n$

- 仮定1と2が成立している仮定

---

# 最小二乗法の２つの考え方

２つの考え方があるが、実質的には同じことをしている

- モーメント法

- 残差二乗和の最小化

---

# モーメント法

モーメント条件という方程式を解くことで、パラメターを推定する方法

**考え方**

- 仮定が成り立っているならば、期待値で構成されるモーメント条件が成り立
- モーメント条件から、パラメターを解ける
- モーメント条件をデータの標本平均で置き換えて推定

---

# モーメント条件

仮定1と2より

$$
 E[U|X] = E[U] = 0
$$

また、平均独立であれば、相関（共分散）もゼロになる

$$
 Cov[X,U] = 0
$$

これらの仮定より、 $X$ と $U$ をかけ合わせたものの期待値もゼロになる

$$
 E[XU] = 0
$$

---

# モーメント条件 2

$E[U] = 0$ と $E[XU] = 0$ という条件に、単回帰モデルを代入する。単回帰モデル $Y=\beta_0 + \beta_1 X + U$ を変形すると

$$
 U = Y-\beta_0 - \beta_1 X
$$
なので、代入して**モーメント条件**を導出する。

$$
 E[Y-\beta_0 - \beta_1 X] = 0 \ \ \ (5.3)
$$
$$
 E[X(Y-\beta_0 - \beta_1 X)] = 0 \ \ \ (5.4)
$$

---

# モーメント条件の解

モーメント条件を、データを所与、パラメターを未知数として解く

式(5.3)を期待値の性質( $E[X+Y] = E[X] + E[Y]$ )を使うと、

$$
 E[Y] - \beta_0 - \beta_1 E[X]
$$

となり、変形すると

$$
 \beta_0 = E[Y] - \beta_1 E[X] \ \ \ ★
$$

となる。

---

# モーメント条件の解 2

式5.4の左辺を展開して期待値の性質を適用

$$
 E[X(Y-\beta_0 - \beta_1 X)] = E[XY] - \beta_0E[X] - \beta_1E[XX]
$$

そして、前スライドの★を $\beta_0$ 代入する。 
なんで？ 未知数２つのうち、一つを"消す"


$$
 = E[XY] - (E[Y] - \beta_1 E[X])E[X] - \beta_1E[XX] \\\\ 
 = E[XY] - E[Y]E[X] - \beta_1(E[XX] - E[X]E[Y]) \\\\
 = Cov[X,Y] - \beta_1V[X]
$$

---

# モーメント条件の解 3

式5.4を見返すと、この式の値が０であるので、

$$
 Cov[X,Y] - \beta_1V[X] = 0
$$

すなわち、

$$
 \beta_1V[X] = Cov[X,Y]
$$

これを $\beta_1$ について解くと

$$
 \beta_1 = \frac{Cov[X,Y]}{V[X]}
$$

---

# モーメント条件の解 4

$\beta_1$ が解けた（=所与であるデータのみで書き表せた）ので、これもう一つのモーメント条件(式5.3)から得られた式★に代入する

$$
 \beta_0 = E[Y]-\frac{Cov[X,Y]}{V[X]}E[X]
$$

このように、母集団分布の期待値（分散・共分散）の関係から、回帰パラメターを計算することができる。

---

# モーメント法による推定

実際には、期待値をデータからの標本分散で置き換える
  - 例： $E[X]$ を $\bar{X}$ で置き換え

$$
 \hat{\beta_1} = \frac{\frac{1}{n}\sum(x_i-\bar{x})(y_i-\bar{y})}{\frac{1}{n}\sum(x_i-\bar{x})^2}
$$

$$
 \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
$$

---

# パラメターの解釈

$\beta_1$ は１次関数の傾き

例：修学年数( $X$ 年)が増えると、年収( $Y$ 万円)がどれだけ増えるか？

「修学年数が１年増えると、年収が $\beta_1$ 万円増える」

--

年収の単位に依存するので、 $Y$ が千円単位で集計されていれば、解釈が変わる

--

単位に依存しない方法があるとすれば？

---

# パラメターの解釈：log-level model

被説明変数を自然対数に変換すると、Xが１単位増えるとYが何%増えるか？を調べることができる

$$
 \ln Y = \beta_0 + \beta_1 X + U
$$

もしXが $\Delta X$ 分増加すると、

$$
 \ln (Y + \Delta Y) = \beta_0 + \beta_1 (X + \Delta X) + U
$$

---

# パラメターの解釈：log-level model

差分を取ると、 $\beta_1 \Delta X$ が差となる

$$
  \ln (Y + \Delta Y) -  \ln Y = \beta_1 \Delta X
$$

この左辺は

$$
   \ln (Y + \Delta Y) -  \ln Y = \ln\left(1 + \frac{\Delta Y}{Y} \right) \fallingdotseq  \frac{\Delta Y}{Y}
$$
つまり、％変化として近似できる。

ログ-レベルモデルでは、「Xの微小な変化(1単位の変化)が、 $\beta_1 \times 100$ %増える」という解釈ができる 

---

# 例 5.1. level model

```{r, echo=TRUE}
dat_income <- read_csv("../data/tanaka_data/csv/5_1_income.csv")
reg5.1 = lm(income ~ yeduc, data=dat_income)

```

$$
  Y = \beta_0 + \beta_1 X + U
$$

$\hat{\beta_1} = 23.151$ → 修学年数が１年伸びると、年収が23.2万円増加

---

# 例 5.1. level model

```{r, echo=TRUE}
summary(reg5.1)
```


---

# 例 5.1. log-level model


```{r, echo=TRUE}

reg5.1_log = lm(lincome ~ yeduc, data=dat_income)

```

$$
  \ln Y = \beta_0 + \beta_1 X + U
$$

$\hat{\beta_1} = 0.065$ → 修学年数が１年伸びると、年収が6.5%増加

---

# 例 5.1. log-level model

```{r, echo=TRUE}
summary(reg5.1_log)
```

---

# 例 5.1. log-log model


```{r, echo=TRUE}

reg5.1_loglog = lm(lincome ~ lyeduc, data=dat_income)

```

$$
  \ln Y = \beta_0 + \beta_1 \ln X + U
$$

$\hat{\beta_1} = 0.8127$ → 修学年数が１%伸びると、年収が0.81%増加

---

# 例 5.1. log-log model

```{r, echo=TRUE}
summary(reg5.1_loglog)
```


---

# 最小二乗法の別解

残差の二乗和を最小にするパラメターを求める

```{r, fig.align='center',fig.height=5}

dat = tibble(
  x = c(2,4,5,7,8),
  y = c(2,6,8,5,6)
)

mod1 <- lm(y~x, dat)

dat <- dat %>%
  mutate(yhat = predict(mod1))

ggplot(dat, aes(x=x,y=y)) +
  geom_point(size=2) + 
  lims(y=c(0,10)) +
  geom_segment(aes(x=x,y=y,xend=x,yend=yhat),col="red") +
  geom_smooth(method="lm",se=FALSE) + 
  theme_bw() +
  theme(text=element_text(size = 20))

```
赤で示された部分が残差: $\hat{u} = y-\hat{y}$

---

# 最小二乗法

残差の二乗和の最小化問題を解くと、回帰パラメターを得られる

$$
 \min_{\hat{\beta_0},\hat{\beta_1}}\sum^{n} \hat{u}_i^2 = \sum^{n} (y_i -\hat{\beta_0} - \hat{\beta_1} x_i)^2 \ \ \ ☆
$$

---

# 最適化問題

.pull-left[

**最小化問題**？

ある関数が最小値を取る状態を解析する問題

微分してゼロ（フラット）なところが(局所的に)最小

]

.pull-right[

```{r}

  fun_quad =  function(x) 2*x^2 + x + 2

  ggplot() +
  xlim(-2, 2) +
  geom_function(fun=fun_quad,size=2) + 
  geom_segment(aes(x=-1,xend=0.5,y=1.8,yend=1.8),col="blue",size=2) +
  geom_segment(aes(x=0.6,xend=1.4,y=3.0,yend=6.8),col="blue",size=2) +
  geom_point(aes(x=c(-0.25,1),y=c(1.9,5)),col="red",size=3) +
  labs(x ="beta", y="残差二乗和") +
  theme_bw(base_family="HiraKakuPro-W3") + 
  theme(text=element_text(size=20))

```


]

---

# 最小二乗法 2

式☆を $\hat{\beta_0},\hat{\beta_1}$ について微分してゼロと置くと、

$$
 \sum (y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 \\\\
 \sum x_i(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0
$$
となり、これはモーメント条件をデータで置き換えたものと同じものとなる。(実際には $n$ を両辺に掛けたものと同じ)

---

# 決定係数 (1)

単回帰で求めた１次関数がどの程度データを説明しているのか？

.pull-left[

```{r,fig.height=5}
dat2 <- tibble(
  x = 1:100,
  y = 5 + 2*x + rnorm(100,0,3),
  y2 = 5 + 2*x + rnorm(100,0,50)
)

ggplot(dat2,aes(x=x,y=y)) + 
  geom_point(size=2) + 
  geom_smooth(method="lm",se=FALSE) +
  theme_bw() +
  theme(text = element_text(size=20))
```


]

.pull-right[

```{r,fig.height=5}
ggplot(dat2,aes(x=x,y=y2)) + 
  geom_point(size=2) + 
  geom_smooth(method="lm",se=FALSE) +
  theme_bw() +
  theme(text = element_text(size=20))
```


]

---

# 決定係数 (2)

被説明変数 $Y$ のばらつき（**総変動**）のうち、どれぐらいが説明変数 $X$ によって説明されるのか？

説明される度合いが高ければ、当てはまりが良い

→総変動にたいする残差二乗和の変動の割合が低い

**決定係数**: 総変動に占める残差変動の割合

$$
 R^2 = 1 - \frac{\sum\hat{u}^2_i}{\sum (y_i - \bar{y})^2}
$$

---

# 例 5.1 決定係数


```{r, echo=TRUE}
summary(reg5.1)
```


---

# 最小二乗法はよい推定量か？

よい推定量とは？

--

**不偏性** : $E[\hat{\beta}]=\beta$

--

**一致性** : $\hat{\beta}_n \rightarrow_p \beta$

--

**効率性** : $V[\hat{\beta}] < V[\hat{\beta}^*]$



---

# 不偏性のための4つの仮定

1. **真のモデル** : 母集団分布において、確率変数 $(Y,X,U)$ の間には次のような関係がある: $Y=\beta_0 + \beta_1X+U$

2. **無作為抽出** : 母集団から標本サイズ $n$ の無作為抽出標本 $\{(x_i,y_i)\}$が得られる

3. **平均独立** : $E[U|X] = 0$

4. **識別条件** : 説明 $X$ に変動がある

---

# 最小二乗推定量の分散 (1)

誤差項が分散均一の場合を仮定

$$
 V[U|X] = V[U] = s^2
$$

傾きパラメターの条件付き分散は誤差項の分散を説明変数の変動で割ったもの

$$
 V[\hat{\beta_1}|x_1,\ldots,x_n] = \frac{n^2}{\sum (x_i - \bar{x})^2}
$$

---

# 最小二乗推定量の分散 (2)

切片パラメターの条件付き分散は

$$
 V[\hat{\beta_0}|x_1,\ldots,x_n] = s^2\left(\frac{1}{n} + \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2}\right)
$$

- 誤差項の分散 $s^2$ が大きいと、推定量の分散が大きくなる
- 説明変数の変動が大きいと、推定量の分散は小さくなる
- 標本サイズが大きいと説明変数の変動が大きくなるので、推定量の分散は小さくなる傾向

---

# 誤差項の分散の推定

誤差項 $U$ は観察されないが、残差 $u_i$ は推定の結果として観察されるため、これを推定値として使う

誤差項の不偏分散は

$$
 \hat{s}^2 = \frac{1}{n-2}\sum\hat{u_i}^2
$$

パラメータが2つあるため、標本サイズ $n$ から2引いたものが、自由度となる

---

# 例 5.3  誤差項の分散


```{r, echo=TRUE}
res = sum(residuals(reg5.1)^2)
print(res)

var = res/(nrow(dat_income)-2)
print(var)

```




---

# OLS is BLUE

最小二乗法(OLS: Ordinary Least Square)はBLUE

**B**est **L**inear **U**nbiased **E**stimator

つまり最良線形不偏推定量

最良とは最も効率的であるということ。

回帰分析の中で最も効率的な線形の推定量であることが、ガウス=マルコフ定理という定理で証明されている

---

# 確認問題 5-1

.pull-left[
 (1) y = -5 + 2x
 
```{r, fig.height=5}
ggplot(data = data.frame(x=0), aes(x=x)) + 
  stat_function(fun=function(x) -5 + 2*x,
                col="blue") + 
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  xlim(-5,5) +
  theme_bw() + 
  theme(text=element_text(size=20))
```

]

.pull-right[

 (2) y = 2 - x

```{r, fig.height=5}
ggplot(data = data.frame(x=0), aes(x=x)) + 
  stat_function(fun=function(x) 2 - x,
                col="blue") + 
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  xlim(-5,5) +
  theme_bw() + 
  theme(text=element_text(size=20))
```

]

---

# 実証分析問題 5-A

```{r,fig.height=5}

regs = list(reg5.1,reg5.1_log,reg5.1_loglog)

modelsummary::msummary(regs,
                       gof_omit = 'Log.Lik.|AIC|BIC|F')

```

---

# 実証分析問題 5-B

```{r, echo=TRUE}

dat_sleep = read_csv("../data/tanaka_data/csv/5_2_sleep.csv")
reg_5.2 = lm(sleep~commute,dat=dat_sleep)
summary(reg_5.2)

```

---

# 実証分析問題 5-C

```{r, echo= TRUE}

dat_abe = read_csv("../data/tanaka_data/csv/5_3_abe.csv")
reg_5.3 = lm(abe~income,dat=dat_abe)
summary(reg_5.3)

```


---
